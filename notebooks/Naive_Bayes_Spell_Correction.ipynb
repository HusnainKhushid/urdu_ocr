{
    "cells": [
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# Naive Bayes Spell Correction\n",
                "\n",
                "This notebook demonstrates a probabilistic approach to spell correction using the **Naive Bayes** rule.\n",
                "\n",
                "## Theory\n",
                "We want to find the correction $c$ that is most likely given the observed typo $w$ (from OCR).\n",
                "\n",
                "$$ \\hat{c} = \\arg\\max_{c} P(c | w) $$\n",
                "\n",
                "Using Bayes' Theorem:\n",
                "\n",
                "$$ P(c | w) = \\frac{P(w | c) P(c)}{P(w)} $$\n",
                "\n",
                "Since $P(w)$ is constant for all candidates $c$, we maximize:\n",
                "\n",
                "$$ \\hat{c} = \\arg\\max_{c} P(w | c) P(c) $$\n",
                "\n",
                "Where:\n",
                "- **$P(c)$ (The Prior)**: The probability that word $c$ appears in the language (Word Frequency).\n",
                "- **$P(w | c)$ (The Likelihood)**: The probability that the user/OCR types $w$ when they meant $c$ (Error Model / Edit Distance)."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 36,
            "metadata": {},
            "outputs": [],
            "source": [
                "import sys\n",
                "import os\n",
                "from collections import Counter\n",
                "import numpy as np\n",
                "\n",
                "# Setup paths\n",
                "sys.path.append(os.path.abspath('..'))\n",
                "DATA_PATH = '../data/urdu_words.txt'"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 1. The Language Model $P(c)$\n",
                "We estimate this using word counts from our corpus."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 37,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "Model loaded with 154781 unique words.\n"
                    ]
                }
            ],
            "source": [
                "class LanguageModel:\n",
                "    def __init__(self, path):\n",
                "        self.words = Counter()\n",
                "        self.N = 0\n",
                "        self.train(path)\n",
                "        \n",
                "    def train(self, path):\n",
                "        if not os.path.exists(path):\n",
                "            print(\"Dataset not found!\")\n",
                "            return\n",
                "        with open(path, 'r', encoding='utf-8') as f:\n",
                "            text = f.read()\n",
                "            tokens = text.split()\n",
                "            self.words.update(tokens)\n",
                "            self.N = sum(self.words.values())\n",
                "            \n",
                "    def P(self, word):\n",
                "        \"Probability of word: Count(word) / Total_Words\"\n",
                "        return self.words[word] / self.N\n",
                "\n",
                "lm = LanguageModel(DATA_PATH)\n",
                "print(f\"Model loaded with {len(lm.words)} unique words.\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 2. The Error Model $P(w | c)$\n",
                "This estimates the chance of an error. We can model this using **Edit Distance**.\n",
                "- Distance 0 (same word): Very high probability (e.g., 0.95)\n",
                "- Distance 1: Lower probability (e.g., 0.04)\n",
                "- Distance 2: Very low probability (e.g., 0.009)\n",
                "\n",
                "Ideally, you would use a 'Confusion Matrix' of character errors (e.g., how often 'p' is OCR'd as 'q'), but for now, we use a simple exponential decay based on Edit Distance."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 38,
            "metadata": {},
            "outputs": [],
            "source": [
                "def edit_distance(s1, s2):\n",
                "    if len(s1) < len(s2):\n",
                "        return edit_distance(s2, s1)\n",
                "    if len(s2) == 0:\n",
                "        return len(s1)\n",
                "    previous_row = range(len(s2) + 1)\n",
                "    for i, c1 in enumerate(s1):\n",
                "        current_row = [i + 1]\n",
                "        for j, c2 in enumerate(s2):\n",
                "            insertions = previous_row[j + 1] + 1\n",
                "            deletions = current_row[j] + 1\n",
                "            substitutions = previous_row[j] + (c1 != c2)\n",
                "            current_row.append(min(insertions, deletions, substitutions))\n",
                "        previous_row = current_row\n",
                "    return previous_row[-1]\n",
                "\n",
                "def error_probability(typo, candidate, dist=None):\n",
                "    if dist is None:\n",
                "        dist = edit_distance(typo, candidate)\n",
                "    \n",
                "    # Simple heuristic model\n",
                "    if dist == 0:\n",
                "        return 0.90\n",
                "    elif dist == 1:\n",
                "        return 0.09\n",
                "    elif dist == 2:\n",
                "        return 0.01\n",
                "    else:\n",
                "        return 1e-10"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 3. The Classifier\n",
                "We generate candidates (valid words within edit dist 1 or 2) and score them."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 39,
            "metadata": {},
            "outputs": [],
            "source": [
                "def generate_candidates(word, vocabulary):\n",
                "    # For efficiency, in a real system we wouldn't scan the whole vocab.\n",
                "    # We would use the 'edits1' and 'edits2' generator approach.\n",
                "    # Here, for demonstration, let's assume we have a way to get 'potential' candidates.\n",
                "    # Let's pretend our generator gave us these:\n",
                "    \n",
                "    # (In a real scenario, use the method from src/read.py to generate close words)\n",
                "    return vocabulary # Placeholder: Real Naive Bayes iterates generated edits, not full vocab\n",
                "\n",
                "def solve(typo, candidates):\n",
                "    scores = {}\n",
                "    for c in candidates:\n",
                "        prior = lm.P(c)\n",
                "        if prior == 0: continue # Ignore unknown words\n",
                "        \n",
                "        dist = edit_distance(typo, c)\n",
                "        likelihood = error_probability(typo, c, dist)\n",
                "        \n",
                "        # P(c|w) ~ P(w|c) * P(c)\n",
                "        scores[c] = likelihood * prior\n",
                "        \n",
                "    # Return best\n",
                "    if not scores:\n",
                "        return typo, {}\n",
                "    return max(scores, key=scores.get), scores\n"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 4. Demonstration\n",
                "Let's test with 'کتاپ' (Typo for 'کتاب')."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 40,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "Candidates for کتاپ: {'کتان', 'کتا', 'کتاب', 'تاپ'}\n",
                        "\n",
                        "Best Correction: کتان\n",
                        "\n",
                        "Detailed Scores (Likelihood * Prior):\n",
                        "کتان: 5.81e-07 (Prior: 6.46e-06, Dist: 1)\n",
                        "کتا: 5.81e-07 (Prior: 6.46e-06, Dist: 1)\n",
                        "کتاب: 5.81e-07 (Prior: 6.46e-06, Dist: 1)\n",
                        "تاپ: 5.81e-07 (Prior: 6.46e-06, Dist: 1)\n"
                    ]
                }
            ],
            "source": [
                "typo = 'کتاپ'\n",
                "\n",
                "# Efficient Candidate Generation (Using Peter Norvig's edits1 logic for demo)\n",
                "def edits1(word):\n",
                "    letters = 'ابپتٹثجچحخدڈذرڑزژسشصضطظعغفقکگلمنںوہیے'\n",
                "    splits     = [(word[:i], word[i:])    for i in range(len(word) + 1)]\n",
                "    deletes    = [L + R[1:]               for L, R in splits if R]\n",
                "    transposes = [L + R[1] + R[0] + R[2:] for L, R in splits if len(R)>1]\n",
                "    replaces   = [L + c + R[1:]           for L, R in splits if R for c in letters]\n",
                "    inserts    = [L + c + R               for L, R in splits for c in letters]\n",
                "    return set(deletes + transposes + replaces + inserts)\n",
                "\n",
                "# Generate candidates that are actually in our dictionary\n",
                "candidates = set(e for e in edits1(typo) if e in lm.words)\n",
                "\n",
                "print(f\"Candidates for {typo}: {candidates}\")\n",
                "\n",
                "best, all_scores = solve(typo, candidates)\n",
                "\n",
                "print(f\"\\nBest Correction: {best}\")\n",
                "\n",
                "print(\"\\nDetailed Scores (Likelihood * Prior):\")\n",
                "for c in candidates:\n",
                "    print(f\"{c}: {all_scores[c]:.2e} (Prior: {lm.P(c):.2e}, Dist: {edit_distance(typo, c)})\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 5. Evaluation on Synthetic Data\n",
                "We evaluate the accuracy on 500 randomly generated typos, similar to the KNN evaluation."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 41,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "Generated 500 pairs.\n",
                        "Running Evaluation...\n",
                        "Accuracy: 68.20%\n",
                        "Time: 0.11s\n"
                    ]
                }
            ],
            "source": [
                "import random\n",
                "import time\n",
                "\n",
                "def generate_typo(word):\n",
                "    if len(word) < 2: return word\n",
                "    urdu_chars = 'ابپتٹثجچحخدڈذرڑزژسشصضطظعغفقکگلمنںوہیے'\n",
                "    op = random.choice(['insert', 'delete', 'replace', 'transpose'])\n",
                "    word = list(word)\n",
                "    idx = random.randint(0, len(word) - 1)\n",
                "    \n",
                "    if op == 'insert': word.insert(idx, random.choice(urdu_chars))\n",
                "    elif op == 'delete': word.pop(idx)\n",
                "    elif op == 'replace': word[idx] = random.choice(urdu_chars)\n",
                "    elif op == 'transpose' and idx < len(word)-1: word[idx], word[idx+1] = word[idx+1], word[idx]\n",
                "        \n",
                "    return \"\".join(word)\n",
                "\n",
                "# Sample 500 words\n",
                "random.seed(42)\n",
                "valid_words = [w for w in lm.words if len(w) > 3]\n",
                "test_set = [(generate_typo(w), w) for w in random.sample(valid_words, 500)]\n",
                "\n",
                "print(f\"Generated {len(test_set)} pairs.\")\n",
                "\n",
                "def get_candidates(typo):\n",
                "    c1 = set(e for e in edits1(typo) if e in lm.words)\n",
                "    return c1 if c1 else {typo} \n",
                "\n",
                "correct_count = 0\n",
                "start_time = time.time()\n",
                "\n",
                "print(\"Running Evaluation...\")\n",
                "for typo, truth in test_set:\n",
                "    candidates = get_candidates(typo)\n",
                "    best_pred, _ = solve(typo, candidates)\n",
                "    \n",
                "    if best_pred == truth:\n",
                "        correct_count += 1\n",
                "\n",
                "duration = time.time() - start_time\n",
                "accuracy = (correct_count / len(test_set)) * 100\n",
                "print(f\"Accuracy: {accuracy:.2f}%\")\n",
                "print(f\"Time: {duration:.2f}s\")"
            ]
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "Python 3",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "file_extension": ".py",
            "mimetype": "text/x-python",
            "name": "python",
            "nbconvert_exporter": "python",
            "pygments_lexer": "ipython3",
            "version": "3.8.5"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 4
}
